sudo apt update

#Posible paso extra para instalar java
sudo apt-get update --fix-missing
sudo apt-get install -f

#Instalación de java
sudo apt install openjdk-8-jdk
java -version; javac -version
vim .bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

#permisos en el ssh
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

#Descargar el packete de hadoop, en este caso descargaremos el 3.3.4
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
tar xzf hadoop-3.3.4.tar.gz
vim .bashrc



#Configuración del bash
#condifuración del video
export HADOOP_HOME=/home/hdoop/hadoop-3.3.4
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
#configuración de la pagina
export HADOOP_HOME=/usr/local/hadoop 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
export HADOOP_INSTALL=$HADOOP_HOME 

echo $HADOOP_HOME

#Configuración del archivo hadoop-env.sh
readlink -f /usr/bin/javac
$ /usr/lib/jvm/java-8-openjdk-amd64/bin/javac
vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

#Configuración del archivo core-site.xml

mkdir tmpdata

#video
<configuration>
<property>
  <name> hadoop.tmp.dir </name>
  <value> /home/hdoop/tmpdata </value>
</property>
<property>
  <name> fs.default.name </name>
  <value> hdfs://127.0.0.1:9000 </value>
</property>
</configuration>
#pagina
<configuration>
<property>
<name>fs.default.name </name>
<value> hdfs://localhost:9000 </value> 
</property> 
</configuration>

#configuracion del archivo hdfs-site.xml
#Con los archivos creados, namenode y datanode

mkdir dfsdata
mkdir dfsdata/namenode
mkdir dfsdata/datanode

#video
<configuration>
<property>
  <name> dfs.name.dir </name>
  <value> /home/hdoop/dfsdata/namenode </value>
</property>
<property>
  <name> dfs.data.dir </name>
  <value> /home/hdoop/dfsdata/datanode </value>
</property>
<property>
  <name> dfs.replication </name>
  <value> 1 </value>
</property>
</configuration>
#pagina
<configuration>
   <property>
      <name>dfs.replication</name>
      <value>1</value>
   </property>
    
   <property>
      <name>dfs.name.dir</name>
      <value>file:///home/hadoop/hadoopinfra/hdfs/namenode </value>
   </property>
    
   <property>
      <name>dfs.data.dir</name> 
      <value>file:///home/hadoop/hadoopinfra/hdfs/datanode </value> 
   </property>       
</configuration>

#Configuración de yarn
#video
<configuration>
<property>
  <name> yarn.nodemanager.aux-services </name>
  <value> mapreduce_shuffle </value>
</property>
<property>
  <name> yarn.nodemanager.aux-services.mapreduce.shuffle.class </name>
  <value> org.apache.hadoop.mapred.ShuffleHandler </value>
</property>
<property>
  <name> yarn.resourcemanager.hostname </name>
  <value> 127.0.0.1 </value>
</property>
<property>
  <name> yarn.acl.enable </name>
  <value> 0 </value>
</property>
<property>
  <name> yarn.nodemanager.env-whitelist </name>
  <value> JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME </value>
</property>
</configuration>
#pagina
<configuration> 
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value> 
</property>  
</configuration>

#configuración del archivo mapred
#video
<configuration>
<property>
  <name> mapreduce.framework.name </name>
  <value> yarn </value>
</property>
</configuration>
#pagina
<configuration> 
<property> 
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

hdfs namenode -format

tenemos que logearnos 2 veces